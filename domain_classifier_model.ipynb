{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  C:\\Users\\Mictlantecuhtli\\Anaconda3\\python.exe -m pip <command> [options]\n",
      "\n",
      "Commands:\n",
      "  install                     Install packages.\n",
      "  download                    Download packages.\n",
      "  uninstall                   Uninstall packages.\n",
      "  freeze                      Output installed packages in requirements format.\n",
      "  list                        List installed packages.\n",
      "  show                        Show information about installed packages.\n",
      "  check                       Verify installed packages have compatible dependencies.\n",
      "  config                      Manage local and global configuration.\n",
      "  search                      Search PyPI for packages.\n",
      "  wheel                       Build wheels from your requirements.\n",
      "  hash                        Compute hashes of package archives.\n",
      "  completion                  A helper command used for command completion.\n",
      "  help                        Show help for commands.\n",
      "\n",
      "General Options:\n",
      "  -h, --help                  Show help.\n",
      "  --isolated                  Run pip in an isolated mode, ignoring\n",
      "                              environment variables and user configuration.\n",
      "  -v, --verbose               Give more output. Option is additive, and can be\n",
      "                              used up to 3 times.\n",
      "  -V, --version               Show version and exit.\n",
      "  -q, --quiet                 Give less output. Option is additive, and can be\n",
      "                              used up to 3 times (corresponding to WARNING,\n",
      "                              ERROR, and CRITICAL logging levels).\n",
      "  --log <path>                Path to a verbose appending log.\n",
      "  --proxy <proxy>             Specify a proxy in the form\n",
      "                              [user:passwd@]proxy.server:port.\n",
      "  --retries <retries>         Maximum number of retries each connection should\n",
      "                              attempt (default 5 times).\n",
      "  --timeout <sec>             Set the socket timeout (default 15 seconds).\n",
      "  --exists-action <action>    Default action when a path already exists:\n",
      "                              (s)witch, (i)gnore, (w)ipe, (b)ackup, (a)bort.\n",
      "  --trusted-host <hostname>   Mark this host as trusted, even though it does\n",
      "                              not have valid or any HTTPS.\n",
      "  --cert <path>               Path to alternate CA bundle.\n",
      "  --client-cert <path>        Path to SSL client certificate, a single file\n",
      "                              containing the private key and the certificate\n",
      "                              in PEM format.\n",
      "  --cache-dir <dir>           Store the cache data in <dir>.\n",
      "  --no-cache-dir              Disable the cache.\n",
      "  --disable-pip-version-check\n",
      "                              Don't periodically check PyPI to determine\n",
      "                              whether a new version of pip is available for\n",
      "                              download. Implied with --no-index.\n",
      "  --no-color                  Suppress colored output\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EWBqw1PtEYOX",
    "outputId": "8b87a6e8-61f0-49d6-e8df-a7ebfe4be73a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Embedding, Dense, Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7grohEB-mWOP"
   },
   "source": [
    "### Loading Data\n",
    "\n",
    "We define some functions to read the training data from a labelled set of 100000 genuine and fake domain names and load it into a tensorflow dataset. This also returns a character-index embedding, which is used later on for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ih3oDDXBFfC3"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "  domains = pd.read_csv('https://github.com/sudo-rushil/CNN-LSTM_Domain_Classifier/blob/master/domains.csv')\n",
    "  domains.drop(['RootObject.subclass'], axis=1, inplace=True)\n",
    "  columns = {'RootObject.class': 'pred', 'RootObject.domain': 'domain'}\n",
    "  domains.rename(columns=columns, inplace=True)\n",
    "  \n",
    "  for i in range(domains.shape[0]):\n",
    "    if domains['pred'][i] == 'legit':\n",
    "      domains['pred'][i] = 0\n",
    "    else:\n",
    "      domains['pred'][i] = 1\n",
    "\n",
    "  return domains[['domain', 'pred']]\n",
    "\n",
    "def shuffle_data(domains):\n",
    "  iter_len = int(domains.shape[0]/2)\n",
    "  for i in range(iter_len):\n",
    "    if i % 2 == 0:\n",
    "      domain, pred = tuple(domains.iloc[i])\n",
    "      sdomain, spred = tuple(domains.iloc[i + iter_len])\n",
    "      domains['domain'][i] = sdomain\n",
    "      domains['pred'][i] = spred\n",
    "      domains['domain'][i + iter_len] = domain\n",
    "      domains['pred'][i + iter_len] = pred\n",
    "  return domains\n",
    "\n",
    "def load_tf_dataset(domains):\n",
    "  vocab = sorted(set(''.join(domains['domain'].to_list())))\n",
    "  char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "  idx2char = np.array(vocab)\n",
    "\n",
    "  lines = []\n",
    "  for i, line in enumerate(domains.iloc[:, 0]):\n",
    "    lines.append([char2idx[c] for c in line])\n",
    "  \n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(lines, padding='post')\n",
    "  targets = np.array(domains.iloc[:, 1], dtype=np.int32)\n",
    "\n",
    "  data = tf.data.Dataset.from_tensor_slices(tensor)\n",
    "  pred = tf.data.Dataset.from_tensor_slices(targets)\n",
    "  dataset = tf.data.Dataset.zip((data, pred))\n",
    "  \n",
    "  return dataset, (char2idx, idx2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6LvqoKm7dOT2"
   },
   "outputs": [],
   "source": [
    "domains = load_data()\n",
    "domains = shuffle_data(domains)\n",
    "dataset, mappings = load_tf_dataset(domains)\n",
    "char2idx, idx2char = mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1P0UvFQEUwjO"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(domains.shape[0]).batch(1000, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WD5gl-hZlX1y"
   },
   "source": [
    "### Building the Model\n",
    "\n",
    "Here, we build the model using Tensorflow's Keras API. It uses an architecture adapted from B. Yu *et al.*, 2018, which first embeds each character as a 128-dimensional vector, passes it through a 1D convolutional layer, runs it through an LSTM, and classifies it with a single dense layer.\n",
    "\n",
    "The 82 in the input dimension comes from the fact that the maximum domain length in the training data is 82 characters, and the 38 comes from the 38 possible characters in a domain name.\n",
    "\n",
    "We train the model for 20 epochs, using Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aUFF_17BTawb"
   },
   "outputs": [],
   "source": [
    "domain_input = Input(shape=(82,), dtype='int32', name='domain_input')\n",
    "embedding = Embedding(input_dim=38, output_dim=128, input_length=82, \n",
    "                      batch_input_shape=[1000, None])(domain_input)\n",
    "conv = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu', strides=1)(embedding)\n",
    "pool = MaxPooling1D(pool_size=2, padding='same')(conv)\n",
    "lstm = LSTM(64, return_sequences=False)(pool)\n",
    "output = Dense(1, activation='sigmoid')(lstm)\n",
    "model = tf.keras.Model(inputs=domain_input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c7gwdgvzUDZv"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "thek24RvXLpE",
    "outputId": "05837b4b-cd80-4594-9d7f-6f3314cf6818"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 112s 1s/step - loss: 0.2549 - accuracy: 0.8866\n",
      "100/100 [==============================] - 113s 1s/step - loss: 0.0186 - accuracy: 0.9952\n",
      "100/100 [==============================] - 112s 1s/step - loss: 0.0130 - accuracy: 0.9962\n",
      "100/100 [==============================] - 113s 1s/step - loss: 0.0096 - accuracy: 0.9970\n",
      "100/100 [==============================] - 112s 1s/step - loss: 0.0089 - accuracy: 0.9972\n",
      "100/100 [==============================] - 111s 1s/step - loss: 0.0111 - accuracy: 0.9967\n",
      "100/100 [==============================] - 111s 1s/step - loss: 0.0066 - accuracy: 0.9981\n",
      "100/100 [==============================] - 112s 1s/step - loss: 0.0044 - accuracy: 0.9987\n",
      "100/100 [==============================] - 112s 1s/step - loss: 0.0057 - accuracy: 0.9981\n",
      "100/100 [==============================] - 112s 1s/step - loss: 0.0060 - accuracy: 0.9981\n",
      "100/100 [==============================] - 111s 1s/step - loss: 0.0038 - accuracy: 0.9988\n",
      "100/100 [==============================] - 110s 1s/step - loss: 0.0034 - accuracy: 0.9991\n",
      "100/100 [==============================] - 112s 1s/step - loss: 0.0024 - accuracy: 0.9993\n",
      "100/100 [==============================] - 113s 1s/step - loss: 0.0024 - accuracy: 0.9994\n",
      "100/100 [==============================] - 116s 1s/step - loss: 0.0029 - accuracy: 0.9992\n",
      "100/100 [==============================] - 119s 1s/step - loss: 0.0025 - accuracy: 0.9993\n",
      "100/100 [==============================] - 118s 1s/step - loss: 0.0021 - accuracy: 0.9995\n",
      "100/100 [==============================] - 114s 1s/step - loss: 0.0028 - accuracy: 0.9992\n",
      "100/100 [==============================] - 114s 1s/step - loss: 0.0030 - accuracy: 0.9992\n",
      "100/100 [==============================] - 114s 1s/step - loss: 0.0034 - accuracy: 0.9990\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "accuracy = []\n",
    "losses = []\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "  history = model.fit(dataset)\n",
    "\n",
    "  accuracy.append(history.history['accuracy'])\n",
    "  losses.append(history.history['loss'])\n",
    "\n",
    "  model.save_weights('domain_class_v1_checkpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FKeaZFc1lJlA"
   },
   "source": [
    "### Getting Predictions\n",
    "\n",
    "These two functions allow us to input a domain name and get the model's prediction of whether it is genuine or fake. It uses the character-index embedding defined above when loading the tensorflow data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M7YIsZanONQJ"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function to get the prediction score from the model for an input domain.\n",
    "'''\n",
    "\n",
    "def get_raw_prediction(domain_name):\n",
    "  name_vec = [char2idx[c] for c in domain_name]\n",
    "  vec = np.zeros((1, 82))\n",
    "  vec[0, :len(domain_name)] = name_vec\n",
    "\n",
    "  prediction = model(vec).numpy().sum()\n",
    "\n",
    "  return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1KVu1ABE_9L"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function to print the prediction for an input domain.\n",
    "'''\n",
    "\n",
    "def get_prediction(domain_name):\n",
    "  name_vec = [char2idx[c] for c in domain_name]\n",
    "  vec = np.zeros((1, 82))\n",
    "  vec[0, :len(domain_name)] = name_vec\n",
    "\n",
    "  prediction = nm(vec).numpy().sum()\n",
    "\n",
    "  if prediction < 0.5:\n",
    "    print('The domain {} is genuine with probability {}'.format(domain_name, round(1 - prediction, 2)))\n",
    "  else:\n",
    "    print('The domain {} is fake with probability {}'.format(domain_name, round(prediction, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMzj9iq-k-Fg"
   },
   "source": [
    "### Testing the model\n",
    "From here, we load some new data to test the model. The fake examples (tdata1) come from the Bambenek DGA feed, and the \n",
    "real examples come from Alexa's top one million websites.\n",
    "\n",
    "Tests run on the first 10000 domains in each test dataset, after shuffling, gave a 98.7% prediction rate for fake domains and 98.8% for real domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7gfz_WnrKrsD"
   },
   "outputs": [],
   "source": [
    "tdata1 = pd.read_csv('https://osint.bambenekconsulting.com/feeds/dga-feed.txt', index_col=False, names=['domain', 'junk', 'junk2'], skiprows=15)\n",
    "tdata1 = tdata1.drop(['junk', 'junk2'], axis=1)\n",
    "tdata2 = pd.read_csv('https://github.com/sudo-rushil/CNN-LSTM_Domain_Classifier/blob/master/top-1m.csv', names=['domain'], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Fx_0Qn5SNaM7",
    "outputId": "de7aaa87-c870-405b-f6a9-bfe72055d7d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9869\n"
     ]
    }
   ],
   "source": [
    "Correct_Fake = 0\n",
    "tdata1 = tdata1.sample(frac=1)\n",
    "\n",
    "for i in range(10000):\n",
    "  pred = get_raw_prediction(tdata1['domain'][i])\n",
    "  if pred >=  0.5:\n",
    "    Correct_Fake += 1\n",
    "\n",
    "print(Correct_Fake/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "66DtYBbgMuPj",
    "outputId": "9ed0dd45-b29c-41d4-c475-8e8cb63a1193"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.988\n"
     ]
    }
   ],
   "source": [
    "Correct_Real = 0\n",
    "tdata2 = tdata2.sample(frac=1)\n",
    "\n",
    "for i in range(1, 10001):\n",
    "  pred = get_raw_prediction(tdata2['domain'][i])\n",
    "  if pred <  0.5:\n",
    "    Correct_Real += 1\n",
    "\n",
    "print(Correct_Real/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prediction('wikipedia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JQoaIaBnx4x4"
   },
   "source": [
    "### Saving and Loading the Model\n",
    "\n",
    "As this model was originally trained on Google Colab, it was saved through Keras serialization as an .h5 file. The predict_domain.py function allows one to pass individual domains to get a prediction. Alternatively, one can load the domain_classifier.h5 file to get the original model."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Domain Classifier Model",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
